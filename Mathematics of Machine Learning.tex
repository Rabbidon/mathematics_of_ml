\documentclass{article}
\usepackage{enumitem}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\newcommand{\chapternumber}{2}
\title{Mathematics for Machine Learning - Questions and Solutions}
\author{Edwin Fennell}
\date{}
\newenvironment{QandA}{\begin{enumerate}[label=\chapternumber.\arabic*]\bfseries\boldmath}
	{\end{enumerate}}
\newenvironment{answered}{\par\bigskip\normalfont\unboldmath}{}
\usepackage{lipsum}
\pagestyle{empty}
\begin{document}
	\maketitle
	
	Note - I'm not going to write out the questions here since they are very, very inefficiently posed and no way am I going to TeX all of that.
	
	\noindent%
	\begin{QandA}
		\item
		\begin{answered}
			a. In order to show that this constitutes a group, we need to show four things:
			\begin{itemize}
				\item Closure - if $a,b\in\mathbb{R}$ then clearly $ab+a+b\in\mathbb{R}$.
				Now, suppose that $ab + a + b=-1$. This rearranges to
				\[a(b+1)=-(b+1)\]
				or 
				\[(a+1)(b+1)=0\]
				Therefore if neither $a$ nor $b$ is equal to -1, $a*b$ also cannot be equal to -1, and therefore * is a valid group operation on $\mathbb{R}\backslash\{-1\}$.
				\item Identity - our identity is 0 since for any $a\in\mathbb{R}\backslash\{-1\}$ we have
				\[a*0 = a\cdot0 + 0 + a = a\]
				\item Inverse - given a fixed $a\in\mathbb{R}\backslash\{-1\}$ we want to solve for $x$ in the following:
				\[a*x = ax + x + a = 0\]
				we rearrange to get
				\[x = \frac{-a}{a+1}\]
				Therefore all elements in $\mathbb{R}\backslash\{-1\}$ have inverses under *
				\item Associativity - we consider the respective values of $(a*b)*c$ and $a*(b*c)$ for arbitrary $a,b,c\in\mathbb{R}\backslash\{-1\}$:
				\[(a*b)*c = (a*b)c + a*b + c = abc + ac + bc + ab + a + b + c\]
				\[(a*(b*c) = a(b*c) + b*c + a = abc + ac + bc + ab + a + b + c\]
				and so we have associativity.
						
			\end{itemize}
				
				Now we need to show that the resulting group is Abelian, but this is clear from the definition of * being completely symmetric in its two operands.
				
				\qed
				
				b. Conveniently, from our proof of associativity we know immediately that
				\[3*x*x = 3x^2 + 3x + 3x + x^2 + x + x + 3 = 4x^2 + 8x + 3\]
				Therefore we need to solve $4x^2 + 8x + 3 = 15$, or rather
				\[4x^2 + 8x - 12 = 4(x^2 + 2x -3) = 4(x+3)(x-1)+0\]
				From this we see that the solutions are exactly $x=1,x=-3$
		\end{answered}
		
		\item
		\begin{answered}
			a. We need to show the four group axioms:
			\begin{itemize}
				\item Closure - By definiton of $\oplus$ the result of its application is a congruence class mod $n$. (Well-posedness is another matter but that isn't asked for here).
				\item Identity - the identity is $\bar{0}$ since
				\[\forall a\in\mathbb{Z}, \bar{a} \oplus \bar{0} = \overline{(a + 0)} = \bar{a}\]
				\item Inverses - the inverse of $\bar{a}$ for any $a\in\mathbb{Z}$ is $\overline{-a}$:
				\[\forall a\in\mathbb{Z}, \bar{a} \oplus \overline{-a} = \overline{(a-a)} = \bar{0}\]
				\item Associativity - we have
				\[\forall a,b,c\in \mathbb{Z}, (\bar{a} \oplus \overline{b})\oplus \overline{c} = \overline{(a+b)}\oplus \overline{c} = \overline{a+b+c}\]
				and also
				\[\forall a,b,c\in \mathbb{Z}, \bar{a} \oplus (\overline{b}\oplus \overline{c}) = \overline{a} \oplus \overline{(b+c)} = \overline{a+b+c}\]
				and so we have associativity. Assuming that the operator $\oplus$ is well-defined, this more or less comes down to "addition is associative".
			\end{itemize}
			Therefore $(\mathbb{Z}_n,\oplus)$ is indeed a group.
			
			b. I'm not going to write out the multiplication table for $\mathbb{Z}_5\backslash\{\overline{0}\}$. I will show that this is a group when I prove the general case in part d of this question. Assuming that it is a group, it is clearly Abelian from the symmetric nature of $\otimes$.
			
			c. Again, I'll use the result from part d. 8 is composite so this is not a group.
			
			d. Suppose that $n$ is composite. Then $\exists$ $a$,$b$ s.t. $1<a,b<n$ and $a\cdot b=n$. Therefore we have 
			\[\overline{a}\otimes\overline{b}=\overline{n}=\overline{0}\]
			Therefore $\mathbb{Z}_n\backslash\{\overline{0}\}$ is not a group since it fails the requirement of closure.
			
			Now, if $n$ is instead prime, then $\mathbb{Z}_n\backslash\{\overline{0}\}$ is a group - we will show this by verifying the group axioms.
			\begin{itemize}
				\item Closure - suppose that $a,b\in\mathbb{Z}\backslash\{\overline{0}\}$. Now, suppose that
				\[ab\equiv 0 \mod{n}\]
				Then $ab=kn$ for some $k\in \mathbb{Z}$. Since $n$ is prime, $a$ and $n$ are coprime, and therefore by Bezout's theorem, $\exists u,v\in\mathbb{Z}$ s.t.
				\[ua+vn=1\]
				Therefore
				\[b = b\cdot 1 = b(ua+vn) = ab\cdot u + bvn = (uk+bv)n\]
				and so we find that $b$ is a multiple of $n$. This is a contradiction since $b\in\mathbb{Z}\backslash\{\overline{0}\}$. Therefore $ab\not\equiv 0 \mod{n}$ and we have 
				\[\overline{a},\overline{b}\neq\overline{0}\implies\overline{ab}\neq\overline{0}\]
				and so we have closure
				\item Identity - the identity is trivially $\overline{1}$
				\item Inverse - for any $\overline{a}\neq\overline{0}$ we have that $a$ and $n$ are coprime. By Bezout's theorem we know that $\exists u,v\in\mathbb{Z}$ s.t.
				\[ua+vn=1\]
				Therefore
				\[\overline{a}\otimes\overline{u}=\overline{au}=\overline{(1-vn)}=\overline{1}\]
				and so we have constructed an inverse for $\overline{a}$
				\item Associativity - exactly the same proof as in part a. Essentially "multiplication is associative".
			\end{itemize}
			Therefore $(\mathbb{Z}_n,\otimes)$ is indeed a group.
		\end{answered}
		
		\item
		\begin{answered}
			Let's check the four group requirements:
			\begin{itemize}
				\item Closure - $\forall x_1,y_1,z_1,x_2,y_2,z_2\in\mathbb{R}$ we have
				\[
				\begin{pmatrix}
					1 & x_1 & z_1 \\ 0 & 1 & y_1 \\ 0 & 0 & 1
				\end{pmatrix}
				\cdot
				\begin{pmatrix}
					1 & x_2 & z_2 \\ 0 & 1 & y_2 \\ 0 & 0 & 1
				\end{pmatrix}
				=
				\begin{pmatrix}
					1 & x_1+x_2 & x_1y_2+z_1+z_2 \\ 0 & 1 & y_1+y_2 \\ 0 & 0 & 1
				\end{pmatrix}
				\]
				so we have closure
				\item Identity - from the above calculation (or simply by knowing what the identity matrix is) we see that 
				\[
				\begin{pmatrix}
					1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1
				\end{pmatrix}
				\in
				\mathcal{G}\]
				and acts as the identity.
				\item Inverses - we see that $\forall x,y,z\in\mathbb{R}$ we have
				\[
				\begin{pmatrix}
					1 & x_1 & z_1 \\ 0 & 1 & y_1 \\ 0 & 0 & 1
				\end{pmatrix}
				\cdot
				\begin{pmatrix}
					1 & -x_1 & x_1y_1-z_1 \\ 0 & 1 & -y_1 \\ 0 & 0 & 1
				\end{pmatrix}
				=
				\begin{pmatrix}
					1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1
				\end{pmatrix}
				\]
				\item Associativity - we can just show this manually. Let
				\[x_1,y_1,z_1,x_2,y_2,z_2,x_3,y_3,z_3\in\mathbb{R}\]
				Then 
				\[
				\left(
				\begin{pmatrix}
					1 & x_1 & z_1 \\ 0 & 1 & y_1 \\ 0 & 0 & 1
				\end{pmatrix}
				\cdot
				\begin{pmatrix}
					1 & x_2 & z_2 \\ 0 & 1 & y_2 \\ 0 & 0 & 1
				\end{pmatrix}
				\right)
				\cdot
				\begin{pmatrix}
					1 & x_3 & z_3 \\ 0 & 1 & y_3 \\ 0 & 0 & 1
				\end{pmatrix}
				\]
				\[=\begin{pmatrix}
					1 & x_1+x_2 & x_1y_2+z_1+z_2 \\ 0 & 1 & y_1+y_2 \\ 0 & 0 & 1
				\end{pmatrix}
				\cdot
				\begin{pmatrix}
					1 & x_3 & z_3 \\ 0 & 1 & y_3 \\ 0 & 0 & 1
				\end{pmatrix}
				\]
				\[=
				\begin{pmatrix}
					1 & x_1+x_2+x_3 & x_1y_2 + x_1y_3 + x_2y_3 + z_1+z_2+z_3 \\ 0 & 1 & y_1+y_2+y_3 \\ 0 & 0 & 1
				\end{pmatrix}\]
				and
				\[
				\begin{pmatrix}
					1 & x_1 & z_1 \\ 0 & 1 & y_1 \\ 0 & 0 & 1
				\end{pmatrix}
				\cdot
				\left(
				\begin{pmatrix}
					1 & x_2 & z_2 \\ 0 & 1 & y_2 \\ 0 & 0 & 1
				\end{pmatrix}
				\cdot
				\begin{pmatrix}
					1 & x_3 & z_3 \\ 0 & 1 & y_3 \\ 0 & 0 & 1
				\end{pmatrix}
				\right)
				\]
				\[=				\begin{pmatrix}
					1 & x_1 & z_1 \\ 0 & 1 & y_1 \\ 0 & 0 & 1
				\end{pmatrix}
				\cdot
				\begin{pmatrix}
					1 & x_2 + x_3 & x_2y_3 + z_2 + z_3 \\ 0 & 1 & y_2 + y_3 \\ 0 & 0 & 1
				\end{pmatrix}
				\]
				\[=
				\begin{pmatrix}
					1 & x_1+x_2+x_3 & x_1y_2 + x_1y_3 + x_2y_3 + z_1+z_2+z_3 \\ 0 & 1 & y_1+y_2+y_3 \\ 0 & 0 & 1
				\end{pmatrix}\]
				and so we have associativity.
			\end{itemize}
			Therefore $(\mathcal{G},\cdot)$ is a group. It is not Abelian though. We can see this from observing that
			\[
			\begin{pmatrix}
				1 & 1 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 1
			\end{pmatrix}
			\cdot
			\begin{pmatrix}
				1 & 1 & 1 \\ 0 & 1 & 0 \\ 0 & 0 & 1
			\end{pmatrix}
			=
			\begin{pmatrix}
				1 & 2 & 2 \\ 0 & 1 & 1 \\ 0 & 0 & 1
			\end{pmatrix}
			\]
			and
			\[
			\begin{pmatrix}
				1 & 1 & 1 \\ 0 & 1 & 0 \\ 0 & 0 & 1
			\end{pmatrix}
			\cdot
			\begin{pmatrix}
				1 & 1 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 1
			\end{pmatrix}
			=
			\begin{pmatrix}
				1 & 2 & 3 \\ 0 & 1 & 1 \\ 0 & 0 & 1
			\end{pmatrix}
			\]
		\end{answered}
		
		\item
		\begin{answered}
			The first product is not computable since the column count of the first matrix is not equal to the row count of the second. All the other products are valid. I will not compute them here.
		\end{answered}
		
		\item
		\begin{answered}
			We could just ask a linear equation solver to do this for us, but I think that the point of this question is to understand the solving process. In each case we use row operations to put the "query matrix" into echelon row form. Note that we can combine the ability to multiply rows by non-zero constants and adding a row to another row to simply add a non-zero multiple of a row to a different row.\\
			\\
			a. We need to solve
			\[\begin{pmatrix}
				1 & 1 & -1 & -1\\
				2 & 5 & -7 & -5\\
				2 & -1 & 1 & 3\\
				5 & 2 & -4 & 2
			\end{pmatrix}\textbf{x}
			=
			\begin{pmatrix}
				1\\
				-2\\
				4\\
				6
			\end{pmatrix}\]
			We add multiples of row 1 to all other rows in order to force as many zeros as possible in the first column. giving
				\[\begin{pmatrix}
				1 & 1 & -1 & -1\\
				0 & 3 & -5 & -3\\
				0 & -3 & 3 & 5\\
				0 & -3 & 1 & 7
			\end{pmatrix}\textbf{x}
			=
			\begin{pmatrix}
				1\\
				-4\\
				2\\
				1
			\end{pmatrix}\]
			We repeat for row 2 and the second column:
			\[\begin{pmatrix}
				1 & 1 & -1 & -1\\
				0 & 3 & -5 & -3\\
				0 & 0 & -2 & 2\\
				0 & 0 & -4 & 4
			\end{pmatrix}\textbf{x}
			=
			\begin{pmatrix}
				1\\
				-4\\
				-2\\
				-3
			\end{pmatrix}\]
			and then row 3 and the third column:
			\[\begin{pmatrix}
				1 & 1 & -1 & -1\\
				0 & 3 & -5 & -3\\
				0 & 0 & -2 & 2\\
				0 & 0 & 0 & 0
			\end{pmatrix}\textbf{x}
			=
			\begin{pmatrix}
				1\\
				-4\\
				-2\\
				1
			\end{pmatrix}\]
			Understandably, $0=1$ has no solutions, so there are no solutions to this equation.
			
			b. We need to solve
			\[\begin{pmatrix}
				1 & -1 & 0 & 0 & 1 \\
				1 & 1 & 0 & -3 & 0 \\
				2 & -1 & 0 & 1 & -1 \\
				-1 & 2 & 0 & -2 & -1
			\end{pmatrix}
			\textbf{x}=
			\begin{pmatrix}
				3\\
				6\\
				5\\
				1
			\end{pmatrix}
			\]
			Again we do the whole "row reduction" thing. We add multiples of the first row the other rows in order to make a bunch of zeroes in the first column:
			\[\begin{pmatrix}
				1 & -1 & 0 & 0 & 1 \\
				0 & 2 & 0 & -3 & -1 \\
				0 & 1 & 0 & 1 & -3 \\
				0 & 1 & 0 & -2 & 0
			\end{pmatrix}
			\textbf{x}=
			\begin{pmatrix}
				3\\
				3\\
				-1\\
				4
			\end{pmatrix}
			\]
			Then we swap the second and third rows:
			\[\begin{pmatrix}
				1 & -1 & 0 & 0 & 1 \\
				0 & 1 & 0 & 1 & -3 \\
				0 & 2 & 0 & -3 & -1 \\
				0 & 1 & 0 & -2 & 0
			\end{pmatrix}
			\textbf{x}=
			\begin{pmatrix}
				3\\
				-1\\
				3\\
				4
			\end{pmatrix}
			\]
			Then we use the second row to introduce zeros into the second column:
			\[\begin{pmatrix}
				1 & -1 & 0 & 0 & 1 \\
				0 & 1 & 0 & 1 & -3 \\
				0 & 0 & 0 & -5 & 5 \\
				0 & 0 & 0 & -3 & 3
			\end{pmatrix}
			\textbf{x}=
			\begin{pmatrix}
				3\\
				-1\\
				5\\
				5
			\end{pmatrix}
			\]
			Multiply the third row by 3 and the last row by 5:
			\[\begin{pmatrix}
				1 & -1 & 0 & 0 & 1 \\
				0 & 1 & 0 & 1 & -3 \\
				0 & 0 & 0 & -15 & 15 \\
				0 & 0 & 0 & -15 & 15
			\end{pmatrix}
			\textbf{x}=
			\begin{pmatrix}
				3\\
				-1\\
				15\\
				25
			\end{pmatrix}
			\]
			and then subtract the third row from the 4th to give
						\[\begin{pmatrix}
				1 & -1 & 0 & 0 & 1 \\
				0 & 1 & 0 & 1 & -3 \\
				0 & 0 & 0 & -15 & 15 \\
				0 & 0 & 0 & 0 & 0
			\end{pmatrix}
			\textbf{x}=
			\begin{pmatrix}
				3\\
				-1\\
				15\\
				10
			\end{pmatrix}
			\]
			Therefore any solution to this equation would require $10=0$, and thus there are no solutions.
		\end{answered}
		
		\item
		\begin{answered}
			We need to solve 
			\[\begin{pmatrix}
				0 & 1 & 0 & 0 & 1 & 0 \\
				0 & 0 & 0 & 1 & 1 & 0 \\
				0 & 1 & 0 & 0 & 0 & 1 \\
			\end{pmatrix}
			\textbf{x}
			=
			\begin{pmatrix}
				2\\
				-1\\
				1
			\end{pmatrix}
			\]
			We can pretty straightforwardly subtract the first row from the third, giving
			\[\begin{pmatrix}
				0 & 1 & 0 & 0 & 1 & 0 \\
				0 & 0 & 0 & 1 & 1 & 0 \\
				0 & 0 & 0 & 0 & -1 & 1 \\
			\end{pmatrix}
			\textbf{x}
			=
			\begin{pmatrix}
				2\\
				-1\\
				-1
			\end{pmatrix}
			\]
			Dealing with inhomogeneity is annoying though, so a simpler practice would be to solve the homogeneous
			\[\begin{pmatrix}
				0 & 1 & 0 & 0 & 1 & 0 \\
				0 & 0 & 0 & 1 & 1 & 0 \\
				0 & 0 & 0 & 0 & -1 & 1 \\
			\end{pmatrix}
			\textbf{x}
			=
			\textbf{0}\]
			It doesn't take too much imagination here to see that the vectors
			\[\begin{pmatrix}
				1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
			\end{pmatrix},
			\begin{pmatrix}
				0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0
			\end{pmatrix},
			\begin{pmatrix}
				0 \\ -1 \\ 0 \\ -1 \\ 1 \\ 1
			\end{pmatrix}
			\]
			are linearly independent and lie in the kernel of this matrix
			It also doesn't take too much imagination to see that
			\[\begin{pmatrix}
				0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0
			\end{pmatrix},
			\begin{pmatrix}
				0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0
			\end{pmatrix},
			\begin{pmatrix}
				0 \\ 0 \\ 0 \\ 0 \\ 0 \\ 1
			\end{pmatrix}
			\]
			are linearly independent, and that the span of their images under  $\textbf{A}$ is all of $\mathbb{R}^3$. Therefore by rank-nullity theorem the kernel of $A$ is exactly
			\[\text{span}\left(
			\begin{pmatrix}
				1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
			\end{pmatrix},
			\begin{pmatrix}
				0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0
			\end{pmatrix},
			\begin{pmatrix}
				0 \\ -1 \\ 0 \\ -1 \\ 1 \\ 1
			\end{pmatrix}
			\right)\]
			Now that we have solved the homogenous equation, we need a specific solution to 
			\[\begin{pmatrix}
				0 & 1 & 0 & 0 & 1 & 0 \\
				0 & 0 & 0 & 1 & 1 & 0 \\
				0 & 0 & 0 & 0 & -1 & 1 \\
			\end{pmatrix}
			\textbf{x}
			=
			\begin{pmatrix}
				2\\
				-1\\
				-1
			\end{pmatrix}
			\]
			in order to get our general solution. We can see from our construction of a basis of $\text{Im}(\textbf{A})$ that
			\[
			\begin{pmatrix}
				0 \\ 2 \\ 0 \\ -1 \\ 0 \\ -1
			\end{pmatrix}
			\]
			is such a solution. Therefore we express the general solution as
			\[\left\{
			\begin{pmatrix}
				0 \\ 2 \\ 0 \\ -1 \\ 0 \\ -1
			\end{pmatrix} + \textbf{y},
			\textbf{y}\in
			\text{span}\left(
			\begin{pmatrix}
				1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0
			\end{pmatrix},
			\begin{pmatrix}
				0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0
			\end{pmatrix},
			\begin{pmatrix}
				0 \\ -1 \\ 0 \\ -1 \\ 1 \\ 1
			\end{pmatrix}
			\right)
			\right\}
			\]
			
		\end{answered}
		
		\item 
		\begin{answered}
			Note that both of the equations posed are linear, so we can combine them into a single linear system describing our three unknowns. This is a little fiddly so I will do so in several separate steps.\\
			\\
			We see that
			\[\textbf{Ax}=12\textbf{x}\]
			rearranges to
			\[(\textbf{A}-12\textbf{I})\textbf{x} = \textbf{0}\]
			Our second requirement can be restated as
			\[\begin{pmatrix}
				1 & 1 & 1
			\end{pmatrix}
			\textbf{x} = 1\]
			We can combine these into a single linear system, giving
			\[
			\begin{pmatrix}
				1 & 1 & 1 \\
				-6 & 4 & 3 \\
				6 & -12 & 9 \\
				0 & 8 & -12 \\
			\end{pmatrix}
							\textbf{x}
			=
			\begin{pmatrix}
				1 \\ 0 \\ 0 \\ 0
			\end{pmatrix}
			\]
			We will use Gaussian elimination to solve this. Reducing the first column:
			\[
			\begin{pmatrix}
				1 & 1 & 1 \\
				0 & 10 & 9 \\
				0 & -18 & 3 \\
				0 & 8 & -12 \\
			\end{pmatrix}
			\textbf{x}
			=
			\begin{pmatrix}
				1 \\ 6 \\ -6 \\ 0
			\end{pmatrix}
			\]
			Swapping rows 2 and 4, then dividing row 2 through by 4:
			\[
			\begin{pmatrix}
				1 & 1 & 1 \\
				0 & 2 & -3 \\
				0 & -18 & 3 \\
				0 & 10 & 9 \\
			\end{pmatrix}
			\textbf{x}
			=
			\begin{pmatrix}
				1 \\ 0 \\ -6 \\ 6
			\end{pmatrix}
			\]
			Now we reduce column 2:
						\[
			\begin{pmatrix}
				1 & 1 & 1 \\
				0 & 2 & -3 \\
				0 & 0 & -24 \\
				0 & 0 & 24 \\
			\end{pmatrix}
			\textbf{x}
			=
			\begin{pmatrix}
				1 \\ 0 \\ -6 \\ 6
			\end{pmatrix}
			\]
			And then add the 3rd row to the 4th to remove it altogether:
			\[
			\begin{pmatrix}
				1 & 1 & 1 \\
				0 & 2 & -3 \\
				0 & 0 & -24 \\
				0 & 0 & 0 \\
			\end{pmatrix}
			\textbf{x}
			=
			\begin{pmatrix}
				1 \\ 0 \\ -6 \\ 0
			\end{pmatrix}
			\]
			Or more succinctly:
			\[
			\begin{pmatrix}
				1 & 1 & 1 \\
				0 & 2 & -3 \\
				0 & 0 & -24 \\
			\end{pmatrix}
			\textbf{x}
			=
			\begin{pmatrix}
				1 \\ 0 \\ -6
			\end{pmatrix}
			\]
			The matrix on the left is upper triangular, and so we can clearly see that the determinant is non-zero, meaning that the matrix is invertible. Computing the inverse is not mathematically interesting so I will let the computer do the work, revealing the unique solution to our equation as
			\[\textbf{x}
			=
			\begin{pmatrix}
				1 & 1 & 1 \\
				0 & 2 & -3 \\
				0 & 0 & -24 \\
			\end{pmatrix}^{-1}
			\begin{pmatrix}
				1 \\ 0 \\ -6
			\end{pmatrix}
			=
			\frac{1}{8}\cdot
			\begin{pmatrix}
				3 \\ 3 \\ 2
			\end{pmatrix}\]
		\end{answered}
		
		\item
		\begin{answered}
			a. The rows of this matrix are not independent - the second row is the arithmetic average of the other two. Therefore the matrix is non-invertible.
			
			b. This one is not quite so obvious from the outset. We could easily find out whether or not the determinant is zero by plugging it into Wolfram Alpha, but let's do this manually instead. Row operations do not change the row space of the matrix, and we will use this to our advantage.
			
			We start by adding multiples of the first row to everything to "zero out" the first column:
			
			\[\begin{pmatrix}
				1 & 0 & 1 & 0 \\
				0 & 1 & 1 & 0 \\
				0 & 1 & -1 & 1 \\
				0 & 1 & 0 & 0 \\
			\end{pmatrix}
			\]
			
			Now we do the same for the second row and column:
			
			\[\begin{pmatrix}
				1 & 0 & 1 & 0 \\
				0 & 1 & 1 & 0 \\
				0 & 0 & -2 & 1 \\
				0 & 0 & -1 & 0 \\
			\end{pmatrix}
			\]
			
			Now we switch rows 3 and 4:
			
			\[\begin{pmatrix}
				1 & 0 & 1 & 0 \\
				0 & 1 & 1 & 0 \\
				0 & 0 & -1 & 0 \\
				0 & 0 & -2 & 1 \\
			\end{pmatrix}
			\]
			
			And now we reduce the third column using the third row:
			
			\[\begin{pmatrix}
				1 & 0 & 1 & 0 \\
				0 & 1 & 1 & 0 \\
				0 & 0 & -1 & 0 \\
				0 & 0 & 0 & 1 \\
			\end{pmatrix}
			\]
			
			The determinant of this upper triangular matrix is -1, so it is invertible. Since the row space is unchanged, the original matrix was also invertible.
			
			At this point I have no qualms about getting Wolfram Alpha to compute the inverse for me, since I know that it exists. Plugging constants into the lengthy formula for a 4x4 inverse is not an instructive or enjoyable exercise.
			
			Thus we find the inverse to be 
			
			\[\begin{pmatrix}
				0 & -1 & 0 & 1 \\
				-1 & 0 & 0 & 1 \\
				1 & 1 & 0 & -1 \\
				1 & 1 & 1 & -2 \\
			\end{pmatrix}\]
		\end{answered}
		
		\item 
		\begin{answered}
			a. Despite appearances, this is a vector space. Note that $f:\mathbb{R}\rightarrow\mathbb{R}$ given by $f(x)=f(x^3)$ is a surjection. Therefore we can rewrite
			\[A = \{(x,x+y,x-y)|x,y\in\mathbb{R}\}\]
			or more succinctly:
			\[A = \left\{
			x\begin{pmatrix}
				1 \\ 1 \\ 1
			\end{pmatrix}
			+			
			y\begin{pmatrix}
				0 \\ 1 \\ -1
			\end{pmatrix}
			|x,y\in\mathbb{R}
			\right\}
			\]
			which is a 2D plane through the origin and thus clearly a subspace of $\mathbb{R}^3$
			
			b. This is not a vector space. $(1,-1,0)\in B$ but $-1*(1,-1,0)=(-1,1,0)\notin B$
			
			c. This is a subspace iff $\gamma=0$. If $\gamma\neq0$ then $(0,0,0)\notin C$, and having an additive identity is a requirement of a vector space.
			
			If $\gamma=0$ then we can pick $\xi_2$ and $\xi_3$ freely, at which point $\xi_1$ is uniquely determined as $2\xi_2-3\xi_3$. Thus we can write $C$ as
			\[A = \left\{
			x\begin{pmatrix}
				2 \\ 1 \\ 0
			\end{pmatrix}
			+			
			y\begin{pmatrix}
				-3 \\ 0 \\ 1
			\end{pmatrix}
			|x,y\in\mathbb{R}
			\right\}
			\]
			which is a plane through the origin. $C$ is a subspace in this case.
			
			d. $D$ is not a subspace since it is not closed under scalar multiplication in $\mathbb{R}$. $(1,1,1)\in D$, but $\frac{1}{2}(1,1,1)=(\frac{1}{2},\frac{1}{2},\frac{1}{2})\notin D$
		\end{answered}
		\item 
		\begin{answered}
			a. The vectors are linearly dependent: $x_3-2x_1=x_2$
			b. These vectors are linearly independent. Suppose that we have
			\[a_1x_1+a_2x_2+a_3x_3=0\]
			Looking at dimension 3, we see that necessarily $a_1=0$. Then, looking at dimension 2, we see that this forces $a_2=0$, which also forces $a_3=0$.
		\end{answered}
		\item
		\begin{answered}
			We want to solve
			\[a_1x_1+a_2x_2+a_3x_3=y\]
			which we can rewrite as
			\[
			\begin{pmatrix}
				1 & 1 & 2 \\
				1 & 2 & -1 \\
				1 & 3 & 1
			\end{pmatrix}
			\begin{pmatrix}
				a_1 \\ a_2 \\ a_3
			\end{pmatrix}
			=
			y
			\]
			If we check we can see that the matrix on the left is invertible. We can therefore compute its inverse and write
			\[
			\begin{pmatrix}
				a_1 \\ a_2 \\ a_3
			\end{pmatrix}
			=
			\frac{1}{5}
			\begin{pmatrix}
				5 &  5 & -5 \\
				-2 & -1 &  3 \\
				1 & -2 &  1 \\
			\end{pmatrix}
			y
			=
			\begin{pmatrix}
				-6 \\ 3 \\ 2
			\end{pmatrix}
			\]
			Therefore we have
			\[-6x_1+3x_2+2x_3=y\]
		\end{answered}
		\item
		\begin{answered}
			Suppose we want to find the generic form of a point $x\in U_1\cap U_2$. Then we can write
			\[x = 
			\begin{pmatrix}
				1 &  2 & -1 \\
				1 & -1 &  1 \\
				-3 &  0 & -1 \\
				1 & -1 &  1 \\
			\end{pmatrix}
			\begin{pmatrix}
				a_1 \\ a_2 \\ a_3
			\end{pmatrix}
			=	
			\begin{pmatrix}
				-1 &  2 & -3 \\
				-2 & -2 &  6 \\
				2 &  0 & -2 \\
				1 &  0 & -1 \\
			\end{pmatrix}
			\begin{pmatrix}
				b_1 \\ b_2 \\ b_3
			\end{pmatrix}
			\]
			for some $a_1,a_2,a_3,b_1,b_2,b_3$. We are not however free to pick these 6 values independently however we want - we do actually need the above equality to hold.
			
			So we need
			\[
			\begin{pmatrix}
				1 &  2 & -1 \\
				1 & -1 &  1 \\
				-3 &  0 & -1 \\
				1 & -1 &  1 \\
			\end{pmatrix}
			\begin{pmatrix}
				a_1 \\ a_2 \\ a_3
			\end{pmatrix}
			=	
			\begin{pmatrix}
				-1 &  2 & -3 \\
				-2 & -2 &  6 \\
				2 &  0 & -2 \\
				1 &  0 & -1 \\
			\end{pmatrix}
			\begin{pmatrix}
				b_1 \\ b_2 \\ b_3
			\end{pmatrix}
			\]
			which we can rewrite as
			\[
			\begin{pmatrix}
				1 &  2 & -1 & -1 &  2 & -3  \\
				1 & -1 &  1  & -2 & -2 &  6\\
				-3 &  0 & -1 & 2 &  0 & -2 \\
				1 & -1 &  1  & 1 &  0 & -1\\
			\end{pmatrix}
			\begin{pmatrix}
				a_1 \\ a_2 \\ a_3 \\ b_1 \\ b_2 \\ b_3
			\end{pmatrix}
			= 0
			\]
			We can now solve this by Gaussian elimination. We reduce column 1 with multiples of the first row:
			\[
			\begin{pmatrix}
				1 &  2 & -1 & -1 &  2 & -3  \\
				0 & -3 &  2  & -1 & -4 &  9\\
				0 &  6 & -4 & -1 &  6 & -11 \\
				0 & -3 &  2  & 2 &  -2 & 2\\
			\end{pmatrix}
			\begin{pmatrix}
				a_1 \\ a_2 \\ a_3 \\ b_1 \\ b_2 \\ b_3
			\end{pmatrix}
			= 0
			\]
			and now we reduce column 2 with multiples of the second row:
			\[
			\begin{pmatrix}
				1 &  2 & -1 & -1 &  2 & -3  \\
				0 & -3 &  2  & -1 & -4 &  9\\
				0 &  0 & 0 & -3 &  -2 & 7 \\
				0 &  0 & 0 & 3 &  2 & -7\\
			\end{pmatrix}
			\begin{pmatrix}
				a_1 \\ a_2 \\ a_3 \\ b_1 \\ b_2 \\ b_3
			\end{pmatrix}
			= 0
			\]
			and now we reduce column 4 with multiples of the third row:
			\[
			\begin{pmatrix}
				1 &  2 & -1 & -1 &  2 & -3  \\
				0 & -3 &  2  & -1 & -4 &  9\\
				0 &  0 & 0 & -3 &  -2 & 7 \\
				0 &  0 & 0 & 0 &  0 & 0\\
			\end{pmatrix}
			\begin{pmatrix}
				a_1 \\ a_2 \\ a_3 \\ b_1 \\ b_2 \\ b_3
			\end{pmatrix}
			= 0
			\]
			We can now work out the full set of solutions from this echelon row form.
			There is a general methodology here that I will lay out rigorously at the end of this chapter, but for now I will just execute this methodology.
			
			Actually, in this case we don't even have to solve this entire set of linear equations. As soon as we solve for $b_1,b_2,b_3$ we have our completely general expression for $x$.
			
			We start from the last non-zero row of the matrix and resolve each row in turn:
			\begin{itemize}
				\item We have $-3b_1-2b_2+7b_3=0$, which allows us to pick $b_1,b_2$ freely, and then this forces $b_3=\frac{3b_1+2b_2}{7}$
			\end{itemize}
			
			We can now stop since we have determined all possible value combinations for $b_1,b_2,b_3$, and so we have the general form for $x$ as
			\[
			\begin{pmatrix}
				-1 &  2 & -3 \\
				-2 & -2 &  6 \\
				2 &  0 & -2 \\
				1 &  0 & -1 \\
			\end{pmatrix}
			\begin{pmatrix}
				b_1 \\ b_2 \\ \frac{3b_1+2b_2}{7}
			\end{pmatrix}
			=
			\frac{1}{7}
			\begin{pmatrix}
				-16b_1+8b_2 \\ 4b_1-2b_2 \\ 8b_1-4b_2 \\ 4b_1-2b_2
			\end{pmatrix}
			=(2b_1-b_2)
			\begin{pmatrix}
				-8 \\ 2 \\ 4 \\ 2
			\end{pmatrix}
			\]
			We can select $b_1,b_2$ to force $2b_1-b_2$ to take any value we like. Therefore the singleton set containing the vector
			\[\begin{pmatrix}
				-8 \\ 2 \\ 4 \\ 2
			\end{pmatrix}
			\]
			is a basis for $U_1\cap U_2$.
		\end{answered}
		
		\item
		\begin{answered}
			a. The easiest way to work this out is to look at the column space, which is exactly the image space. Then via rank-nullity theorem
			\[dim(ker(A_1))+dim(Im(A_1)) = dim(Preim(A_1))=3\]
			For $A_1$ we see that the third column is the sum of the first two, and that the first two columns are not linearly dependent. Therefore $dim(Im(A_1))=2$, and $dim(ker(A_1))=dim(U_1)=1$.
			
			For $A_2$, we also see that the third column is the sum of the first two, and that the first two columns are not linearly dependent. Therefore $dim(Im(A_2))=2$, and $dim(ker(A_2))=dim(U_2)=1$.
			
			b. From our observations before, we see that
			\[\begin{pmatrix}
				1 \\ 1 \\ -1
			\end{pmatrix}\]
			lies in both $U_1$ and $U_2$. The dimension of each of these spaces is 1, so therefore the singleton set containing only this vector acts as a basis for both $U_1$ and $U_2$.
			
			c. We have $U_1=U_2=U_1\cap U_2$, so
			\[\left\{
			\begin{pmatrix}
				1 \\ 1 \\ -1
			\end{pmatrix}
			\right\}\]
			is also a basis for $=U_1\cap U_2$
		\end{answered}
		
		\item
		\begin{answered}
			a. We already know that the column space has dimension 2 for each of the above matrices: $dim(U_1)=dim(U_2)=2$
			b. We also know from the previous question that the first two columns of each matrix generate its column space. So
			\[\left\{
			\begin{pmatrix}
				1 \\ 1 \\ 2 \\ 1
			\end{pmatrix},
			\begin{pmatrix}
				0 \\ -2 \\ 1 \\ 0
			\end{pmatrix}
			\right\}\]
			is a basis of $U_1$ and
						\[\left\{
			\begin{pmatrix}
				3 \\ 1 \\ 7 \\ 3
			\end{pmatrix},
			\begin{pmatrix}
				-3 \\ 2 \\ -5 \\ -1
			\end{pmatrix}
			\right\}\]
			is a basis of $U_2$.
			
			c. We note that 
			\[\begin{pmatrix}
				3 \\ 1 \\ 7 \\ 3
			\end{pmatrix}\]
			lies in $U_1$ but that
			\[\begin{pmatrix}
				-3 \\ 2 \\ -5 \\ -1
			\end{pmatrix}\]
			does not. Therefore the dimension of $U_1\cap U_2$ is at least 1 but less than 2, since it does not contain all of $U_2$. Therefore the dimension is exactly 1 and we can write our basis of $U_1\cap U_2$ as
			\[\left\{
			\begin{pmatrix}
				3 \\ 1 \\ 7 \\ 3
			\end{pmatrix}
			\right\}\]
		\end{answered}
		
		\item 
		\begin{answered}
			a. In order to see whether a set is a subspace of a space, we need to check closure under addition and scalar multiplication - all the other requirements for being a vector space are inherited from the parent space.
			
			For $F$ we can check these two properties directly:
			\begin{itemize}
				\item Closure under addition - suppose that we have
				\[(x_1,y_1,z_1),(x_2,y_2,z_2)\in F\]
				Then we also have
				\[(x_1+x_2,y_1+y_2,z_1+z_2)\in F\]
				since
				\[(x_1+x_2) + (y_1+y_2) - (z_1+z_2) = (x_1+y_1-z_1) + (x_2+y_2-z_2) = 0 + 0 = 0\]
				\item Closure under scalar multiplication - suppose that we have
				\[(x,y,z)\in F\]
				Then for any $\lambda\in\mathbb{R}$ we have
				\[(\lambda x,\lambda y,\lambda z)\in F\]
				since 
				\[(\lambda x+\lambda y-\lambda z) = \lambda(x+y-z) = \lambda\cdot0 = 0\]
			\end{itemize}
			
			With $G$ it is a lot easier to see from the construction that it is a 2-dimensional subspace and work out a fairly obvious basis, but we'll handle it in the same way we did $F$ for the sake of practice.
			\begin{itemize}
				\item Closure under addition - suppose that we have (completely generally)
				\[(a_1-b_1,a_1+b_1,a_1-3b_1),(a_2-b_2,a_2+b_2,a_2-3b_2)\in G\]
				then the sum of these two elements, which we call $v$, is also in $G$.
				We can see this since if we set $A=a_1+a_2$ and $B=b_1+b_2$, then we have
				\[v = (A-B,A+B,A-3B)\] and so $v\in G$.
				\item Closure under scalar multiplication - suppose that we have
				\[(a-b,a+b,a-3b)\in G\] and some $\lambda\in\mathbb{R}$. Set $A=\lambda a,B=\lambda b$ and we see that
				\[(\lambda(a-b),\lambda(a+b),\lambda(a-3b)) = (A-B,A+B,A-3B)\in G\]
			\end{itemize}
			Therefore both $F$ and $G$ are subspaces of $\mathbb{R}^3$.
			
			b. Pick a completely general element of $G$, namely $(a-b,a+b,a-3b)$ for some $a,b\in\mathbb{R}$. This element also lies in $F$, and therefore in $F\cap G$, exactly when $a-b+a+b-a+3b=0$, or rather $a=-3b$. Therefore the general element of $F\cap G$ can be written as $\{(-4b,-2b,-6b)|b\in\mathbb{R}\}$.
			
			c. Really? Ok then ...
			
			We can see that $F$ is a 2D space. It contains the mutually linearly independent vectors $(1,0,1)$ and $(1,1,2)$, and doesn't include all of $\mathbb{R}^3$. Therefore $\{(1,0,1),(1,1,2)\}$ is in fact a basis of $F$.
			
			The way that $G$ is defined makes it very clear that $\{(1,1,1),(-1,1,-3)\}$ is a basis of $G$. 
			
			Now that we have a basis for $F$ and $G$ we can say that the general element $v$ of $F\cap G$ satisfies
			\[v
			= a
			\begin{pmatrix}
				1 \\
				0 \\
				1 \\
			\end{pmatrix}
			+ b
			\begin{pmatrix}
				1 \\
				1 \\
				2 \\
			\end{pmatrix}
			=
			c
			\begin{pmatrix}
				1 \\
				1 \\
				1 \\
			\end{pmatrix}
			+ d
			\begin{pmatrix}
				-1 \\
				1 \\
				-3 \\
			\end{pmatrix}
			\]
			We want to find all possible values for $a,b,c,d$. To do this we rewrite the above equation as
			\[
			\begin{pmatrix}
				1 & 1 & -1 & 1 \\
				0 & 1 & -1 & -1 \\
				1 & 2 & -1 & 3 \\
			\end{pmatrix}
			\begin{pmatrix}
				a \\ b \\ c \\ d 
			\end{pmatrix}
			= 0
			\]
			Now we use row operations to put this into echelon row form. We reduce the first column:
			\[
			\begin{pmatrix}
				1 & 1 & -1 & 1 \\
				0 & 1 & -1 & -1 \\
				0 & 1 & 0 & 2 \\
			\end{pmatrix}
			\begin{pmatrix}
				a \\ b \\ c \\ d 
			\end{pmatrix}
			= 0
			\]
			Then the second:
			\[
			\begin{pmatrix}
				1 & 1 & -1 & 1 \\
				0 & 1 & -1 & -1 \\
				0 & 0 & 1 & 3 \\
			\end{pmatrix}
			\begin{pmatrix}
				a \\ b \\ c \\ d 
			\end{pmatrix}
			= 0
			\]
			From this we see that we have completely free choice over the value of $d$, at which point $a,b,c$ are completely determined. In particular we see that $c = -3d$, and so our general form for $v$ is
			\[v=
			c
			\begin{pmatrix}
				1 \\
				1 \\
				1 \\
			\end{pmatrix}
			+ d
			\begin{pmatrix}
				-1 \\
				1 \\
				-3 \\
			\end{pmatrix}
			=
			-3d
			\begin{pmatrix}
				1 \\
				1 \\
				1 \\
			\end{pmatrix}
			+ d
			\begin{pmatrix}
				-1 \\
				1 \\
				-3 \\
			\end{pmatrix}
			= d
			\begin{pmatrix}
				-4 \\
				-2 \\
				-6 \\
			\end{pmatrix}\]
			which is exactly the answer we got in part b.
		\end{answered}
		
		\item 
		\begin{answered}
			a. Yes - integration distributes over addition and commutes with scalar multiplication
			
			b. Yes - differentiation distributes over addition and commutes with scalar multiplication
			
			c. No - $\cos(0)\neq 0$ so $\forall x, \cos(x+0)\neq \cos(x)+\cos(0)$
			
			d. Yes - matrix multiplication distributes over addition and commutes with scalar multiplication
			
			e. No - $\Phi(x)\neq 0$ (where in this case 0 is notation abuse meaning the 2x2 zero matrix) so we have the same problem as in part c.
		\end{answered}
		
		\item
		\begin{answered}
			a. The transformation matrix is 
			\[
			\begin{pmatrix}
				3 & 2 & 1 \\
				1 & 1 & 1 \\
				1 & -3 & 0 \\
				2 & 3 & 1
			\end{pmatrix}
			\]
			b. The columns of this matrix form a linearly independent set, so the rank of the transformation is 3.
			
			c.We already know that
			\[\left\{\begin{pmatrix}
				3 \\
				1 \\
				1 \\
				2
			\end{pmatrix},
			\begin{pmatrix}
				2 \\
				1 \\
				-3 \\
				3
			\end{pmatrix}
			,
			\begin{pmatrix}
				1 \\
				1 \\
				0 \\
				1
			\end{pmatrix}\right\}\]
			is a basis for our image, which has dimension 3. By rank-nullity we see that the dimension of the kernel must be 0, and therefore is the trivial subspace.
		\end{answered}
		
		\item 
		\begin{answered}
			$f$ and $g$ are both automorphisms so they both have trivial kernel and full image, as do $f\circ g$ and $g\circ f$. Therefore all the claimed statements are vacuously true.
		\end{answered}
		
		\item
		\begin{answered}
			a. This square matrix has determinant $-2\neq0$, and therefore has full rank and trivial kernel.
			
			b. Here I am going to go out of my way and explain the whole "basis change" thing in detail in order achieve a clear understanding.
			
			In essence a matrix represents a linear map between two vector spaces. But it's a little more strict than that - a matrix defines a linear map between two vector spaces using a basis of each space. By specifying where each of the basis vectors of the domain space go in terms of our image basis, we define our transformation matrix.
			
			Let's be more specific to this case - we have the same basis for both our domain and image, which we can write as $\{v_1,v_2,v_3\}$. Pick an arbitrary vector in our domain $x = a_1v_1+a_2v_2+a_3v_3$. Then we find unique $b_1,b_2.b_3$ s.t.
			\[A_\Phi x = A_\Phi(a_1v_1+a_2v_2+a_3v_3) = b_1v_1+b_2v_2+b_3v_3\]
			Now, let $B$ denote the invertible square matrix with $v_1,v_2,v_3$ as its columns. We can use this to immediately rewrite the above equation as
			\[A_\Phi B
			\begin{pmatrix}
				a_1 \\ a_2 \\ a_3
			\end{pmatrix}
			=
			B
			\begin{pmatrix}
				b_1 \\ b_2 \\ b_3
			\end{pmatrix}
			\]
			$B$ is invertible so we can pre-multiply by $B^{-1}$ to get
			\[B^{-1}A_\Phi B
			\begin{pmatrix}
				a_1 \\ a_2 \\ a_3
			\end{pmatrix}
			=
			\begin{pmatrix}
				b_1 \\ b_2 \\ b_3
			\end{pmatrix}
			\]
			The above construction is completely general, and thus we have found our target matrix $\bar{A}_\Phi = B^{-1}A_\Phi B$, since it defines where every vector in the domain, expressed in terms of our chosen basis, is mapped to i  the image, also expressed in terms of our chosen basis.
		\end{answered}
		
		\item 
		\begin{answered}
			a. Each pair of vectors is clearly linearly independent and thus span $\mathbb{R}^2$. I'm not going to draw them because I can't be bothered with TeX diagrams.
			
			b. I could just write the answer here, but this is again an opportunity for detail and clarity. Suppose we have two bases $B$ and $B'$ of a space $A$, and we pick an arbitrary point $v\in A$. Then let $x_1,...,x_n$ be the coefficients of $v$ in $B$ and $y_1,...,y_n$ be the coefficients of $v$ in $B'$. Then if we define some $M_i$ as the square matrices with their columns as the basis vectors of $B_i$, we can see that
			\[
			M_1
			\begin{pmatrix}
				x_1 \\ ... \\ x_n
			\end{pmatrix}
			=
			M_2
			\begin{pmatrix}
				y_1 \\ ... \\ y_n
			\end{pmatrix}
			\]
			and therefore
			\[
			\begin{pmatrix}
				x_1 \\ ... \\ x_n
			\end{pmatrix}
			=
			M_1^{-1}
			M_2
			\begin{pmatrix}
				y_1 \\ ... \\ y_n
			\end{pmatrix}
			\]
			Our $v$ was arbitrary so this holds completely generally, and thus our "basis change" matrix from $B'$ to $B$ is $M_1^{-1}M_2$.
			
			In our specific case we have
			\[M_1 = 
			\begin{pmatrix}
				2 & -1 \\
				1 & -1
			\end{pmatrix},
			M_2 = 
			\begin{pmatrix}
				2 & 1 \\
				-2 & 1
			\end{pmatrix}
			\]
			and so we compute our basis change matrix as 
			\[M_1^{-1}M_2=
			\begin{pmatrix}
				4 & 0 \\
				6 & -1
			\end{pmatrix}
			\]
			
			c. i. I'm not doing this yet again. It's a pretty easy computation.
			
			c. ii. Here we get payoff from working out the completely general result int part b. Our specific values for $M_1$ and $M_2$ in this case  are
			\[M_1 = I, M_2 = 
			\begin{pmatrix}
				1 & 0 & 1 \\
				2 & -1 & 0\\
				-1 & 2 & -1				
			\end{pmatrix}\]
			and so our basis change matrix is just $M_2$. (In fact the basis change matrix from any arbitrary basis to the standard basis is just the matrix with the vectors of the original basis making up the columns)
			
			d. To work out $A_\Phi$ w.r.t $B$ and $C$, we should work out where $\Phi$ actually sends all the basis vectors of $B$. Since $\Phi$ is linear we see that
			\[\Phi(b_1)=\frac{1}{2}(\Phi(b_1+b_2)+\Phi(b_1-b_2)) = c_1+2c_3\]
			and
			\[\Phi(b_2)=\frac{1}{2}(\Phi(b_1+b_2)-\Phi(b_1-b_2)) = -c_1+c_2-c_3\]
			Therefore the transformation matrix $A_\Phi$ w.r.t $B$ as our domain basis and $C$ as our image basis is
			\[\begin{pmatrix}
				1 & -1 \\
				0 & 1 \\
				2 & -1
			\end{pmatrix}\]
			
			e. We can think of the linear map in the required bases as the composition of several transforms that we have already computed. First convert our domain representation from basis $B'$ to basis $B$, then apply the transform $\Phi$, and then convert our image representation from basis $C$ to basis $C'$. We have already computed all three of these matrices and so we can just compose them in order to get the required transformation matrix as
			\[
			\begin{pmatrix}
				1 & 0 & 1 \\
				2 & -1 & 0\\
				-1 & 2 & -1				
			\end{pmatrix}
			\begin{pmatrix}
				1 & -1 \\
				0 & 1 \\
				2 & -1
			\end{pmatrix}
			\begin{pmatrix}
				4 & 0 \\
				6 & -1
			\end{pmatrix} = 
			\begin{pmatrix}
			0 & 2 \\
			-10 & 3 \\
			12 & -4
			\end{pmatrix}\]
			
			f. I'm not going through this rigmarole. The first three parts just involve applying the three constituent transformations in turn. The fourth is just applying the combined transform all at once.
		\end{answered}
	\end{QandA}
	$\textbf{Key points for Chapter 2}$
	\begin{itemize}
		
		\item We find it beneficial to define the concept of a vector space. This is a bunch of "things" with an addition function and scalar multiplication (over some field), with closure under each. This is really abstract and general but whatever - this is just the maths that we're developing to use later.
		
		\item We can introduce the concept of dimension - what is the smallest number of "vectors" in this space that we need to be able to generate the entire space with linear combinations thereof. There are absolutely infinite-dimensional vector spaces (consider the field $\mathbb{Q}$ over the vector space $\mathbb{R}$) but we don't care about them. We call any of these sets of generating vectors a "basis".
		
		\item In maths we don't just care about things - we care about maps between things. We can define a linear map as something that "respects linearity" - i.e. we map from one vector space to another, both over the same field, with a map that distributes over addition and commutes with scalar multiplication.
		
		\item Suppose we have a linear map $\Phi$ from space $A$ of dimension $m$ to space $B$ of dimension $n$. Then if we pick any basis of space A, then the action of $\Phi$ on these $m$ basis vectors uniquely defines $\Phi$. Each of these is mapped to a unique linear combination of the image basis. Therefore with respect to a domain and image basis, we can completely define a linear transformation with just $m*n$ variables. This is how we arrive at the concept of a matrix.
		
		\item Note that a matrix is the realisation of an abstract linear map into a specific choice of domain and image basis. However, when it comes to commonly used vector spaces like $\mathbb{R}^n$, there is a canonical basis we often use.
		
		\item Matrix addition doesn't really have a useful geometric interpretation, but matrix multiplication does. We can multiply matrices together to "pre-jack" our calculations and reduce the amount of work we have to do.
		
		\item We can also use matrices to solve systems or linear equations, but possibly a better way to look at this is "what things can we put into this transformation to get the output we want". We will see how this becomes useful later - ML is all about tailoring a system to get specific outputs under certain circumstances.
		
		\item I like to describe the general solution to a matrix equation (or linear system, if you prefer) as a specific non-homogeneous solution plus the general solution to the homogenous problem.
		
		\item In general the best way to do this is via row reductions. We are allowed to switch rows, multiply through by non-zero constants, and add any multiple of a row to a distinct row. Note that all of these operations are completely reversible. This guarantees that the solution set is unaffected.
		
		\item It is fairly easy to see how we can use the above to achieve echelon row form. We work down the rows of the matrix, and across the columns. For each column if all remaining rows have 0 element skip to the next column. Else switch rows to get a row with non-zero element. Scale so that element is 1. Add multiples of this row to all rows below it s.t. the column element becomes 0. Move onto the next column and the next row. Stop when we run off the right or bottom of the matrix.
		
		\item If we are attempting to achieve a specific inhomogeneous solution then we need to "update" our target vector each time we perform an operation. If we are trying for a general homogeneous solution then we don't.
		
		\item After we do all this it is relatively easy to obtain a solution. Start with the bottom row. We have free choice for each "newly chosen" variable apart from the last one, which is determined by our choice of all previous variables. We write dependent variables in terms of our independent ones. This will give us the general solution for the homogeneous equation, and will give a solution for the inhomogeneous one assuming that there isn't a row of all zeros in the matrix and a non-zero in the target vector.
		
		\item We can make this even easier with reduced row echelon form - where all pivots are 1 and all elements in the pivot column other than the pivot are 0. We start from the bottom row and work up here - note that this is pretty much analogous to a big chunk of the solving process described above. It seems like the programmatic way they solve this is called the "minus 1 trick".
		
		\item Previously we used row reduction to solve a system of linear equations, but it can also be used to determine a matrix inverse from $AA^{-1}=I$. As we do row operations on A to convert it into reduced echelon row form, the final result will be the identity matrix, and the matrix on the right will be warped into the actual numerical $A^{-1}$.
		
		\item In reality there are better practical ways to solve linear systems and Gaussian elimination isn't used.
		
		\item This chapter has some other information but I don't really care. The above are the important things I didn't know all that well, expressed in a good way.
	\end{itemize}
\end{document}